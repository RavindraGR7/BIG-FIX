{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8MMZH2zTHnRqAkV4RQzTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RavindraGR7/BIG-FIX/blob/main/final_project_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vgXwpTKze8xg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "monthly_data = pd.read_csv('monthly_data.csv', parse_dates=['Date'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Sharpe Ratio and add it to the DataFrame\n",
        "monthly_data['Sharpe_Ratio'] = monthly_data['Avg_Daily_Return'] / monthly_data['Std_Daily_Return']\n"
      ],
      "metadata": {
        "id": "aiFWtqDxi7wj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(monthly_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZCJT7J4jFtB",
        "outputId": "7abeabaf-5eb6-4003-aaee-e88083874901"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date       Open       High        Low      Close  Adj Close  \\\n",
            "0 2011-01-31  11.992857  12.144286  11.939286  12.118571  10.259578   \n",
            "1 2011-02-28  12.544286  12.680357  12.540000  12.614643  10.679551   \n",
            "2 2011-03-31  12.370000  12.492857  12.359286  12.446786  10.537444   \n",
            "3 2011-04-30  12.385000  12.641071  12.381071  12.504643  10.586424   \n",
            "4 2011-05-31  12.182143  12.422500  12.178571  12.422500  10.516882   \n",
            "\n",
            "       Volume  Daily_Return  Dollar_Volume  Avg_Daily_Return  ...  \\\n",
            "0   377246800      0.051960   3.870393e+09          0.024222  ...   \n",
            "1   403074000      0.040935   4.304649e+09          0.024222  ...   \n",
            "2   274019200     -0.013306   2.887462e+09          0.024222  ...   \n",
            "3  1006345200      0.004648   1.065360e+10          0.024222  ...   \n",
            "4   417754400     -0.006569   4.393474e+09          0.024222  ...   \n",
            "\n",
            "   Skewness_Daily_Return  Avg_Dollar_Volume  Std_Dollar_Volume  \\\n",
            "0              -0.082208       8.311242e+09       5.894129e+09   \n",
            "1              -0.082208       8.311242e+09       5.894129e+09   \n",
            "2              -0.082208       8.311242e+09       5.894129e+09   \n",
            "3              -0.082208       8.311242e+09       5.894129e+09   \n",
            "4              -0.082208       8.311242e+09       5.894129e+09   \n",
            "\n",
            "   Cumulative_Return_3M  Cumulative_Return_6M  Cumulative_Return_9M  \\\n",
            "0              0.127384              0.319028              0.299629   \n",
            "1              0.135176              0.452942              0.375000   \n",
            "2              0.080450              0.228229              0.385560   \n",
            "3              0.031858              0.163300              0.361049   \n",
            "4             -0.015232              0.117885              0.430811   \n",
            "\n",
            "   Cumulative_Return_12M  Symbol Next_Month_Return  Sharpe_Ratio  \n",
            "0               0.766740    AAPL          0.040935      0.307451  \n",
            "1               0.726175    AAPL         -0.013306      0.307451  \n",
            "2               0.483021    AAPL          0.004648      0.307451  \n",
            "3               0.341032    AAPL         -0.006569      0.307451  \n",
            "4               0.354056    AAPL         -0.034960      0.307451  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_data.sort_values(by=['Symbol', 'Date'], inplace=True)\n"
      ],
      "metadata": {
        "id": "zmYyZtHLjldC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = monthly_data[(monthly_data['Date'] >= '2010-01-01') & (monthly_data['Date'] <= '2023-12-31')]\n"
      ],
      "metadata": {
        "id": "SLbH6egyjmcE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem-1"
      ],
      "metadata": {
        "id": "oEcTAbH7kHvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('monthly_data.csv', parse_dates=['Date'])\n",
        "data.sort_values(by=['Symbol', 'Date'], inplace=True)\n",
        "\n",
        "# Drop rows where the target or any predictors are NaN\n",
        "data.dropna(subset=['Next_Month_Return'] + [col for col in data.columns if col != 'Next_Month_Return'], inplace=True)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "X = data[numeric_cols].drop(columns=['Next_Month_Return'])\n",
        "y = data['Next_Month_Return']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
        "\n",
        "# Normalization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "S6vIY6qXjoo8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define a range of regularization parameters to test\n",
        "parameters = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
        "\n",
        "# Setting up the grid search to find the best regularization parameter\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='r2', cv=5)\n",
        "ridge_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best regularization parameter\n",
        "print(\"Best regularization parameter (alpha):\", ridge_regressor.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbOT1TWkR0l",
        "outputId": "45dcd9bd-9748-48ee-be07-36e893bd4128"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best regularization parameter (alpha): {'alpha': 100.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_train_pred = ridge_regressor.predict(X_train_scaled)\n",
        "y_test_pred = ridge_regressor.predict(X_test_scaled)\n",
        "\n",
        "# Calculating R-squared and MSE for the training set\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculating R-squared and MSE for the testing set\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"Training R-squared:\", train_r2)\n",
        "print(\"Training Mean Squared Error:\", train_mse)\n",
        "print(\"Testing R-squared:\", test_r2)\n",
        "print(\"Testing Mean Squared Error:\", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cawM089kljc",
        "outputId": "84bbe5a7-04e0-4c90-b930-ca68cf2a64b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training R-squared: 0.032862156401918585\n",
            "Training Mean Squared Error: 0.005550480985205499\n",
            "Testing R-squared: -0.07192183145781272\n",
            "Testing Mean Squared Error: 0.005015988187866907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a broader and more granular range of alpha values\n",
        "alpha_range = [1e-3, 1e-2, 1e-1, 1, 10, 20, 50, 75, 100, 150, 200]\n",
        "\n",
        "# Setup the grid search\n",
        "ridge_regressor = GridSearchCV(ridge, {'alpha': alpha_range}, scoring='r2', cv=5)\n",
        "ridge_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best alpha found and corresponding R-squared\n",
        "print(\"Best regularization parameter (alpha):\", ridge_regressor.best_params_)\n",
        "best_model = ridge_regressor.best_estimator_\n",
        "print(\"Best CV R-squared:\", ridge_regressor.best_score_)\n",
        "\n",
        "# Reevaluate the model with the new alpha\n",
        "y_train_pred = best_model.predict(X_train_scaled)\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Training R-squared:\", r2_score(y_train, y_train_pred))\n",
        "print(\"Testing R-squared:\", r2_score(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWLsanPhlOFT",
        "outputId": "101e84b2-4991-49d4-bb3c-e6a7471f015f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best regularization parameter (alpha): {'alpha': 200}\n",
            "Best CV R-squared: -0.03449180580924634\n",
            "Training R-squared: 0.028640399027446417\n",
            "Testing R-squared: -0.054963226881706184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem-2"
      ],
      "metadata": {
        "id": "yjlKAaZSmzhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data is already loaded and split into X_train, X_test, y_train, y_test\n",
        "# Normalize data (repeat from the previous setups if not already applied)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "dtree = DecisionTreeRegressor(random_state=8)\n",
        "\n",
        "# Create the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
        "    'min_samples_split': [2, 10, 20, 30],\n",
        "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Set up the grid search\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Output the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation R-squared: \", grid_search.best_score_)\n",
        "\n",
        "# Assess the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_train_pred = best_model.predict(X_train_scaled)\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Print model performance\n",
        "print(\"Training R-squared: \", train_r2)\n",
        "print(\"Testing R-squared: \", test_r2)\n",
        "print(\"Training MSE: \", train_mse)\n",
        "print(\"Testing MSE: \", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAkYLQYbm0nY",
        "outputId": "b1633d40-b206-4260-e334-d7a853cf5ac2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'max_depth': None, 'max_leaf_nodes': 10, 'min_samples_leaf': 2, 'min_samples_split': 20}\n",
            "Best cross-validation R-squared:  -0.15499717444972788\n",
            "Training R-squared:  0.18818428901088036\n",
            "Testing R-squared:  -0.22034690751547514\n",
            "Training MSE:  0.004659074915911118\n",
            "Testing MSE:  0.005710533635528854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data is already loaded and split into X_train, X_test, y_train, y_test\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "random_forest = RandomForestRegressor(random_state=8)\n",
        "\n",
        "# Create a coarser parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],  # Reduced number of options\n",
        "    'max_depth': [None, 20],  # Focused on potentially more impactful depths\n",
        "    'min_samples_leaf': [1, 4],  # Coarser steps\n",
        "    'min_samples_split': [2, 20],  # Covering extremes\n",
        "    'max_leaf_nodes': [None, 50]  # Simplified to extreme and a middle point\n",
        "}\n",
        "\n",
        "# Set up the grid search with fewer folds\n",
        "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Output the best parameters and best cross-validation score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation R-squared: \", grid_search.best_score_)\n",
        "\n",
        "# Assess the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_train_pred = best_model.predict(X_train_scaled)\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Print model performance\n",
        "print(\"Training R-squared: \", train_r2)\n",
        "print(\"Testing R-squared: \", test_r2)\n",
        "print(\"Training MSE: \", train_mse)\n",
        "print(\"Testing MSE: \", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_jzyXf2oQa9",
        "outputId": "b6638668-667e-46d9-a04c-122d90ab0543"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'max_depth': None, 'max_leaf_nodes': None, 'min_samples_leaf': 4, 'min_samples_split': 20, 'n_estimators': 100}\n",
            "Best cross-validation R-squared:  -0.07936488611891845\n",
            "Training R-squared:  0.4663920996659332\n",
            "Testing R-squared:  -0.1431584496583609\n",
            "Training MSE:  0.003062418169204132\n",
            "Testing MSE:  0.0053493352892610225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine the model with new parameters\n",
        "random_forest_adjusted = RandomForestRegressor(\n",
        "    n_estimators=200,  # increased from 100\n",
        "    max_depth=30,       # a reasonable cap to prevent very deep trees\n",
        "    min_samples_leaf=4,\n",
        "    min_samples_split=20,\n",
        "    max_leaf_nodes=None,  # consider setting a cap here if overfitting persists\n",
        "    random_state=8\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_forest_adjusted.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the adjusted model\n",
        "y_train_pred = random_forest_adjusted.predict(X_train_scaled)\n",
        "y_test_pred = random_forest_adjusted.predict(X_test_scaled)\n",
        "print(\"Adjusted Training R-squared: \", r2_score(y_train, y_train_pred))\n",
        "print(\"Adjusted Testing R-squared: \", r2_score(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv6VnLghq4Pk",
        "outputId": "56df78aa-cfa6-401f-c357-0d82b8773e9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Training R-squared:  0.469623956438076\n",
            "Adjusted Testing R-squared:  -0.15209188816492247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem-4"
      ],
      "metadata": {
        "id": "OmPcVfFCq671"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "eWgl7ITMq6mg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "nn_model = MLPRegressor(random_state=8)\n"
      ],
      "metadata": {
        "id": "uZk7uuUhrHBg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'learning_rate_init': [0.001, 0.01],\n",
        "    'max_iter': [200]  # You can adjust this depending on the convergence\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=nn_model, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Output the best parameters and best cross-validation score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation R-squared: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTZx3WN-rIr9",
        "outputId": "6dd0f54a-a232-4c2f-81de-c7c9626e12d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'activation': 'relu', 'hidden_layer_sizes': (100, 50), 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'adam'}\n",
            "Best cross-validation R-squared:  -0.24791467491192995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess the best model\n",
        "best_nn_model = grid_search.best_estimator_\n",
        "y_train_pred = best_nn_model.predict(X_train_scaled)\n",
        "y_test_pred = best_nn_model.predict(X_test_scaled)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Print model performance\n",
        "print(\"Training R-squared: \", train_r2)\n",
        "print(\"Testing R-squared: \", test_r2)\n",
        "print(\"Training MSE: \", train_mse)\n",
        "print(\"Testing MSE: \", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz2XwjDdrKZJ",
        "outputId": "8843f20a-977c-439e-f5a7-e8ba1d279a66"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training R-squared:  0.12418881335173626\n",
            "Testing R-squared:  -0.20304085924163195\n",
            "Training MSE:  0.005026350039241806\n",
            "Testing MSE:  0.005629551113135226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(100, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "# Setup early stopping\n",
        "early_stopping_monitor = EarlyStopping(patience=5)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=200,\n",
        "                    callbacks=[early_stopping_monitor], verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_mse = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
        "test_loss, test_mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(\"Training MSE:\", train_mse)\n",
        "print(\"Testing MSE:\", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UFUy9bkrnYH",
        "outputId": "8c39d843-6ad5-4a24-c486-9cbe7b526e97"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "13/13 [==============================] - 1s 21ms/step - loss: 0.0668 - mean_squared_error: 0.0668 - val_loss: 0.0154 - val_mean_squared_error: 0.0154\n",
            "Epoch 2/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
            "Epoch 3/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
            "Epoch 4/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
            "Epoch 5/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
            "Epoch 6/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
            "Epoch 7/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
            "Epoch 8/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
            "Epoch 9/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
            "Epoch 10/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
            "Epoch 11/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
            "Epoch 12/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
            "Epoch 13/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
            "Epoch 14/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
            "Epoch 15/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
            "Epoch 16/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
            "Epoch 17/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
            "Epoch 18/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
            "Training MSE: 0.0037758788093924522\n",
            "Testing MSE: 0.00587070919573307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem-6"
      ],
      "metadata": {
        "id": "mQsbAKKstwio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Assuming 'data' is your original DataFrame\n",
        "# Select numeric columns for interaction terms\n",
        "numeric_cols = monthly_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Exclude the target variable from feature interactions\n",
        "features = monthly_data[numeric_cols].drop(columns=['Next_Month_Return'])\n",
        "\n",
        "# Generate polynomial and interaction features\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
        "features_poly = poly.fit_transform(features)\n",
        "features_poly_df = pd.DataFrame(features_poly, columns=poly.get_feature_names_out(features.columns))\n",
        "\n",
        "# Combine with original data\n",
        "enhanced_data = pd.concat([monthly_data.drop(columns=numeric_cols), features_poly_df], axis=1)\n"
      ],
      "metadata": {
        "id": "Mw-dlBkhtzDj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_data = pd.concat([monthly_data[['Next_Month_Return']].reset_index(drop=True), features_poly_df], axis=1)\n",
        "\n",
        "# Now check the columns to make sure 'Next_Month_Return' is included\n",
        "print(enhanced_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtlUPUNju8_L",
        "outputId": "26c14672-274a-48d4-fc8e-287397efe72e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Next_Month_Return       Open       High        Low      Close  Adj Close  \\\n",
            "0           0.040935  11.992857  12.144286  11.939286  12.118571  10.259578   \n",
            "1          -0.013306  12.544286  12.680357  12.540000  12.614643  10.679551   \n",
            "2           0.004648  12.370000  12.492857  12.359286  12.446786  10.537444   \n",
            "3          -0.006569  12.385000  12.641071  12.381071  12.504643  10.586424   \n",
            "4          -0.034960  12.182143  12.422500  12.178571  12.422500  10.516882   \n",
            "\n",
            "         Volume  Daily_Return  Dollar_Volume  Avg_Daily_Return  ...  \\\n",
            "0  3.772468e+08      0.051960   3.870393e+09          0.024222  ...   \n",
            "1  4.030740e+08      0.040935   4.304649e+09          0.024222  ...   \n",
            "2  2.740192e+08     -0.013306   2.887462e+09          0.024222  ...   \n",
            "3  1.006345e+09      0.004648   1.065360e+10          0.024222  ...   \n",
            "4  4.177544e+08     -0.006569   4.393474e+09          0.024222  ...   \n",
            "\n",
            "   Cumulative_Return_6M^2  Cumulative_Return_6M Cumulative_Return_9M  \\\n",
            "0                0.101779                                   0.095590   \n",
            "1                0.205156                                   0.169853   \n",
            "2                0.052089                                   0.087996   \n",
            "3                0.026667                                   0.058959   \n",
            "4                0.013897                                   0.050786   \n",
            "\n",
            "   Cumulative_Return_6M Cumulative_Return_12M  \\\n",
            "0                                    0.244612   \n",
            "1                                    0.328915   \n",
            "2                                    0.110240   \n",
            "3                                    0.055690   \n",
            "4                                    0.041738   \n",
            "\n",
            "   Cumulative_Return_6M Sharpe_Ratio  Cumulative_Return_9M^2  \\\n",
            "0                           0.098085                0.089777   \n",
            "1                           0.139257                0.140625   \n",
            "2                           0.070169                0.148657   \n",
            "3                           0.050207                0.130357   \n",
            "4                           0.036244                0.185598   \n",
            "\n",
            "   Cumulative_Return_9M Cumulative_Return_12M  \\\n",
            "0                                    0.229737   \n",
            "1                                    0.272316   \n",
            "2                                    0.186234   \n",
            "3                                    0.123129   \n",
            "4                                    0.152531   \n",
            "\n",
            "   Cumulative_Return_9M Sharpe_Ratio  Cumulative_Return_12M^2  \\\n",
            "0                           0.092121                 0.587890   \n",
            "1                           0.115294                 0.527331   \n",
            "2                           0.118541                 0.233310   \n",
            "3                           0.111005                 0.116303   \n",
            "4                           0.132453                 0.125356   \n",
            "\n",
            "   Cumulative_Return_12M Sharpe_Ratio  Sharpe_Ratio^2  \n",
            "0                            0.235735        0.094526  \n",
            "1                            0.223263        0.094526  \n",
            "2                            0.148505        0.094526  \n",
            "3                            0.104850        0.094526  \n",
            "4                            0.108855        0.094526  \n",
            "\n",
            "[5 rows x 190 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the data\n",
        "X = enhanced_data.drop(columns=['Next_Month_Return'])\n",
        "y = enhanced_data['Next_Month_Return']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
        "\n",
        "\n",
        "X_train = X_train[y_train.notnull()]\n",
        "y_train = y_train[y_train.notnull()]\n",
        "\n",
        "# Since X_train was likely transformed with a scaler, make sure to apply transformations again if necessary\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "sHgs5sxjuPoU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Define and train the Ridge Regression model\n",
        "best_alpha = 100\n",
        "ridge = Ridge(alpha=best_alpha)  # Use the best alpha found earlier or tune again if needed\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = ridge.predict(X_train_scaled)\n",
        "y_test_pred = ridge.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Training R-squared:\", r2_score(y_train, y_train_pred))\n",
        "print(\"Testing R-squared:\", r2_score(y_test, y_test_pred))\n",
        "print(\"Training MSE:\", mean_squared_error(y_train, y_train_pred))\n",
        "print(\"Testing MSE:\", mean_squared_error(y_test, y_test_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwoBicNevEIr",
        "outputId": "a3479090-228d-4c6f-b415-6375b9c760e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training R-squared: 0.10995697982751274\n",
            "Testing R-squared: -0.027278345639673462\n",
            "Training MSE: 0.004921935214873224\n",
            "Testing MSE: 0.005684073934259992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem-7"
      ],
      "metadata": {
        "id": "fVaYHG1qxM3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define parameters for financial metrics (make sure this matches your actual use)\n",
        "params = {\n",
        "    'profitability': ('NetIncomeLoss', 'Assets'),\n",
        "    'liquidity': ('AssetsCurrent', 'LiabilitiesCurrent'),\n",
        "    'activity': ('Revenues', 'Assets'),\n",
        "    'financial_leverage': ('Liabilities', 'Assets')\n",
        "}\n",
        "\n",
        "def process_financial_data(data, params):\n",
        "    results = []\n",
        "    if not data or 'facts' not in data or 'us-gaap' not in data['facts']:\n",
        "        return results  # Ensure we're returning a list even if empty\n",
        "\n",
        "    us_gaap = data['facts']['us-gaap']\n",
        "    for key, (num, den) in params.items():\n",
        "        num_data = us_gaap.get(num, {}).get('units', {}).get('USD', [])\n",
        "        den_data = us_gaap.get(den, {}).get('units', {}).get('USD', [])\n",
        "        if num_data and den_data:\n",
        "            num_df = pd.DataFrame(num_data)\n",
        "            den_df = pd.DataFrame(den_data)\n",
        "            num_df['end'] = pd.to_datetime(num_df['end'])\n",
        "            den_df['end'] = pd.to_datetime(den_df['end'])\n",
        "            merged_df = pd.merge(num_df, den_df, on='end', suffixes=('_num', '_den'))\n",
        "            merged_df[key] = merged_df['val_num'] / merged_df['val_den']\n",
        "            results.append(merged_df[['end', key]])\n",
        "        else:\n",
        "            print(f\"Data for {key} not found or incomplete.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main execution loop to fetch and process data for all companies\n",
        "companies = {'AAPL': '0000320193', 'MSFT': '0000789019', 'GOOGL': '0001652044', 'AMZN': '0001018724'}\n",
        "all_financials = pd.DataFrame()\n",
        "for ticker, cik in companies.items():\n",
        "    raw_data = fetch_financial_data(cik)\n",
        "    financials = process_financial_data(raw_data, params)\n",
        "    for df in financials:\n",
        "        if isinstance(df, pd.DataFrame):  # Check if df is a DataFrame\n",
        "            df['ticker'] = ticker  # Assign ticker\n",
        "            all_financials = pd.concat([all_financials, df], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Expected a DataFrame, got {type(df)} instead.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYoo7qpMx3B8",
        "outputId": "bd407842-8943-4f78-915a-4520bb4c2125"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for activity not found or incomplete.\n",
            "Data for financial_leverage not found or incomplete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define headers to access the SEC API\n",
        "headers = {'User-Agent': 'your_email@example.com'}  # Replace with your email\n",
        "\n",
        "# Companies and their CIK numbers\n",
        "companies = {\n",
        "    'AAPL': '0000320193',\n",
        "    'MSFT': '0000789019',\n",
        "    'GOOGL': '0001652044',\n",
        "    'AMZN': '0001018724'\n",
        "}\n",
        "\n",
        "# Financial parameters to calculate ratios\n",
        "params = {\n",
        "    'profitability': ('NetIncomeLoss', 'Assets'),\n",
        "    'liquidity': ('AssetsCurrent', 'LiabilitiesCurrent'),\n",
        "    'activity': ('Revenues', 'Assets'),\n",
        "    'financial_leverage': ('Liabilities', 'Assets')\n",
        "}\n",
        "\n",
        "def fetch_financial_data(cik):\n",
        "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    return None\n",
        "\n",
        "def process_financial_data(data, params):\n",
        "    results = {}\n",
        "    if not data or 'facts' not in data or 'us-gaap' not in data['facts']:\n",
        "        return results\n",
        "\n",
        "    us_gaap = data['facts']['us-gaap']\n",
        "    for key, (num, den) in params.items():\n",
        "        num_data = us_gaap.get(num, {}).get('units', {}).get('USD', [])\n",
        "        den_data = us_gaap.get(den, {}).get('units', {}).get('USD', [])\n",
        "\n",
        "        if num_data and den_data:\n",
        "            num_df = pd.DataFrame(num_data).rename(columns={'val': 'num_val'})\n",
        "            den_df = pd.DataFrame(den_data).rename(columns={'val': 'den_val'})\n",
        "\n",
        "            num_df['end'] = pd.to_datetime(num_df['end'])\n",
        "            den_df['end'] = pd.to_datetime(den_df['end'])\n",
        "\n",
        "            merged_df = pd.merge(num_df, den_df, on='end', suffixes=('_num', '_den'))\n",
        "            merged_df[key] = merged_df['num_val'] / merged_df['den_val']\n",
        "            results[key] = merged_df[['end', key]]\n",
        "\n",
        "    return results\n",
        "\n",
        "# Fetch and process data for all companies\n",
        "for company, cik in companies.items():\n",
        "    data = fetch_financial_data(cik)\n",
        "    if data:\n",
        "        processed_data = process_financial_data(data, params)\n",
        "        # Save each company's data to CSV\n",
        "        for ratio_name, df in processed_data.items():\n",
        "            df.to_csv(f\"{company}_{ratio_name}.csv\", index=False)\n",
        "    else:\n",
        "        print(f\"Failed to fetch data for {company}\")\n"
      ],
      "metadata": {
        "id": "r7r3I692pPmv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the directory and the scope of companies and ratios\n",
        "directory = './'\n",
        "companies = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
        "ratios = ['profitability', 'liquidity', 'activity', 'financial_leverage']\n",
        "\n",
        "# Function to process a single company's ratios\n",
        "def process_company_ratios(company):\n",
        "    consolidated_ratios = pd.DataFrame()\n",
        "\n",
        "    for ratio in ratios:\n",
        "        file_path = f\"{directory}{company}_{ratio}.csv\"\n",
        "        try:\n",
        "            print(f\"Processing {file_path}...\")\n",
        "            ratio_data = pd.read_csv(file_path)\n",
        "            ratio_data['end'] = pd.to_datetime(ratio_data['end'])\n",
        "\n",
        "            # Filtering data within the date range and handling data properly\n",
        "            filtered_data = ratio_data[(ratio_data['end'] >= '2010-01-01') & (ratio_data['end'] <= '2023-12-31')].copy()\n",
        "            if not filtered_data.empty:\n",
        "                filtered_data.rename(columns={'end': 'Date', ratio_data.columns[1]: f'{company}_{ratio}'}, inplace=True)\n",
        "\n",
        "                if consolidated_ratios.empty:\n",
        "                    consolidated_ratios = filtered_data[['Date', f'{company}_{ratio}']]\n",
        "                else:\n",
        "                    consolidated_ratios = pd.merge(consolidated_ratios, filtered_data[['Date', f'{company}_{ratio}']], on='Date', how='outer')\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "\n",
        "    return consolidated_ratios\n",
        "\n",
        "# Process each company one at a time\n",
        "for company in companies:\n",
        "    company_data = process_company_ratios(company)\n",
        "    if not company_data.empty:\n",
        "        company_data.sort_values(by='Date', inplace=True)\n",
        "        company_data.to_csv(f\"{directory}consolidated_{company}_financial_ratios.csv\", index=False)\n",
        "        print(f\"Consolidated financial ratios for {company} saved successfully.\")\n",
        "    else:\n",
        "        print(f\"No data was consolidated for {company}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r96XFZy8rMEc",
        "outputId": "ca0bdb82-f587-4756-fe4b-7e4745904942"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./AAPL_profitability.csv...\n",
            "Processing ./AAPL_liquidity.csv...\n",
            "Processing ./AAPL_activity.csv...\n",
            "Processing ./AAPL_financial_leverage.csv...\n",
            "Consolidated financial ratios for AAPL saved successfully.\n",
            "Processing ./GOOGL_profitability.csv...\n",
            "Processing ./GOOGL_liquidity.csv...\n",
            "Processing ./GOOGL_activity.csv...\n",
            "Processing ./GOOGL_financial_leverage.csv...\n",
            "Consolidated financial ratios for GOOGL saved successfully.\n",
            "Processing ./MSFT_profitability.csv...\n",
            "Processing ./MSFT_liquidity.csv...\n",
            "Processing ./MSFT_activity.csv...\n",
            "Processing ./MSFT_financial_leverage.csv...\n",
            "Consolidated financial ratios for MSFT saved successfully.\n",
            "Processing ./AMZN_profitability.csv...\n",
            "Processing ./AMZN_liquidity.csv...\n",
            "Processing ./AMZN_activity.csv...\n",
            "File not found: ./AMZN_activity.csv\n",
            "Processing ./AMZN_financial_leverage.csv...\n",
            "File not found: ./AMZN_financial_leverage.csv\n",
            "Consolidated financial ratios for AMZN saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to merge ratios for a specific company\n",
        "def merge_company_data(company):\n",
        "    # Load the monthly data\n",
        "    monthly_data = pd.read_csv('monthly_data.csv')\n",
        "    monthly_data['Date'] = pd.to_datetime(monthly_data['Date'])\n",
        "\n",
        "    try:\n",
        "        # Load the consolidated financial ratios for the company\n",
        "        company_ratios = pd.read_csv(f'./consolidated_{company}_financial_ratios.csv')\n",
        "        company_ratios['Date'] = pd.to_datetime(company_ratios['Date'])\n",
        "\n",
        "        # Merge with the monthly data on 'Date'\n",
        "        merged_data = pd.merge(monthly_data, company_ratios, on='Date', how='left')\n",
        "        print(f\"{company} ratios merged successfully.\")\n",
        "\n",
        "        # Save the merged dataset\n",
        "        merged_data.to_csv(f'fully_merged_monthly_data_{company}.csv', index=False)\n",
        "        print(f\"Merged data for {company} saved successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Consolidated ratio file for {company} not found. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error while merging data for {company}: {str(e)}\")\n",
        "\n",
        "# List of companies\n",
        "companies = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
        "\n",
        "# Call the function for each company\n",
        "for company in companies:\n",
        "    merge_company_data(company)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGI9133fwR32",
        "outputId": "d8f2c121-d6c9-4d1e-863a-1c110d08c581"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAPL ratios merged successfully.\n",
            "Merged data for AAPL saved successfully.\n",
            "GOOGL ratios merged successfully.\n",
            "Merged data for GOOGL saved successfully.\n",
            "MSFT ratios merged successfully.\n",
            "Merged data for MSFT saved successfully.\n",
            "AMZN ratios merged successfully.\n",
            "Merged data for AMZN saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of companies\n",
        "companies = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
        "\n",
        "# Initialize an empty DataFrame to hold all data\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# Iterate through each company and append their data\n",
        "for company in companies:\n",
        "    file_path = f'fully_merged_monthly_data_{company}.csv'\n",
        "    company_data = pd.read_csv(file_path)\n",
        "    all_data = pd.concat([all_data, company_data], ignore_index=True)\n",
        "\n",
        "# Optionally, you can sort by date if it's necessary for your analysis\n",
        "all_data.sort_values(by='Date', inplace=True)\n",
        "\n",
        "# Save the combined dataset\n",
        "all_data.to_csv('fully_merged_monthly_data_all_companies.csv', index=False)\n",
        "print(\"All company data merged into a single file successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEc1X0WqxjFF",
        "outputId": "4218c34a-b52b-417e-f387-f6c0f6b9aba2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All company data merged into a single file successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the consolidated data\n",
        "data = pd.read_csv('fully_merged_monthly_data_all_companies.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Prepare features and target\n",
        "# Assuming 'Next_Month_Return' is the target variable and the rest are features\n",
        "# You may need to adjust these according to your actual data columns\n",
        "X = data.drop(['Date', 'Next_Month_Return'], axis=1)\n",
        "y = data['Next_Month_Return']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define and train the Ridge Regression model\n",
        "best_alpha = 100  # Use the best alpha found earlier or tune again if needed\n",
        "ridge = Ridge(alpha=best_alpha)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = ridge.predict(X_train_scaled)\n",
        "y_test_pred = ridge.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Training R-squared:\", r2_score(y_train, y_train_pred))\n",
        "print(\"Testing R-squared:\", r2_score(y_test, y_test_pred))\n",
        "print(\"Training MSE:\", mean_squared_error(y_train, y_train_pred))\n",
        "print(\"Testing MSE:\", mean_squared_error(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "id": "tckx1Hc00JA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the consolidated data\n",
        "data = pd.read_csv('fully_merged_monthly_data_all_companies.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Strategy 1: Filter based on a more recent date range, e.g., last 5 years\n",
        "filtered_data = data[data['Date'] >= pd.to_datetime(\"2018-01-01\")]\n",
        "\n",
        "# Strategy 3: Random sampling - sample 50% of the data\n",
        "sampled_data = filtered_data.sample(frac=0.5, random_state=8)\n",
        "\n",
        "# Proceed with the sampled data\n",
        "X = sampled_data.drop(['Date', 'Next_Month_Return'], axis=1)\n",
        "y = sampled_data['Next_Month_Return']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
        "\n",
        "# Continue with feature scaling and model training as before...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "flMNqUwu2vIV",
        "outputId": "2d9c267f-2603-4938-b8ba-f8c7a2d5f4d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bc232aba046>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the consolidated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fully_merged_monthly_data_all_companies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m     \"\"\"\n\u001b[1;32m   1337\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}